---
title: "Climate And Statistics"
author: "Helmut Küchenhoff, Henri Funk"
date: "`r Sys.Date()`"
documentclass: krantz
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
colorlinks: yes
lot: False
lof: False
site: bookdown::bookdown_site
description: "A Seminar about statistical methods in climate research in SS24."
graphics: yes
---
<!--- cover-image: images/cover.png -->

```{r setup, include=FALSE}
options(
  htmltools.dir.version = FALSE, formatR.indent = 2, width = 55, digits = 4
)
output <- knitr::opts_knit$get("rmarkdown.pandoc.to")
is.html = !is.null(output) && output == "html"
```

# Preface {-}

*Author: Henri Funk*

```{r cover, cache=FALSE, out.width="500", fig.align="center", echo=FALSE, eval = TRUE}
knitr::include_graphics('cover.jpg')
```

As the world faces the reality of climate change, natural hazards and extreme weather events have become a major concern, with devastating consequences for nature and humans. The quantification and definition of climate change, extreme events and its implications for life and health on our planet is one of the major concerns in climate science. 

This book explains current statistical methods in climate science and their application.
The methods include compound events, low flow events and return periods, natural variability, teleconnections and causal discovery.
All of those methods are used to quantify and anticipate the changing climate.

This book is the outcome of the seminar "Climate and Statistics" which took place in summer 2024 at the Department of Statistics, LMU Munich.

![Creative Commons License](by-nc-sa.png)

This book is licensed under the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).


\mainmatter

# Foreword {-}

*Author: Christoph Molnar*

<!-- An experiment -->
This book is the result of an experiment in university teaching.
Each semester, students of the Statistics Master can choose from a selection of seminar topics.
Usually, every student in the seminar chooses a scientific paper, gives a talk about the paper and summarizes it in the form of a seminar paper.
The supervisors help the students, they listen to the talks, read the seminar papers, grade the work and then ... hide the seminar papers away in (digital) drawers.
This seemed wasteful to us, given the huge amount of effort the students usually invest in seminars.
An idea was born:
Why not create a book with a website as the outcome of the seminar?
Something that will last at least a few years after the end of the semester.
In the summer term 2019, some Statistics Master students signed up for our seminar entitled "Limitations of Interpretable Machine Learning".
When they came to the kick-off meeting, they had no idea that they would write a book by the end of the semester.

We were bound by the examination rules for conducting the seminar, but otherwise we could deviate from the traditional format.
We deviated in several ways:

1. Each student project is part of a book, and not an isolated seminar paper.
1. We gave challenges to the students, instead of papers. The challenge was to investigate a specific limitation of interpretable machine learning methods.
1. We designed the work to live beyond the seminar.
1. We emphasized collaboration. Students wrote some chapters in teams and reviewed each others texts.

<!-- Our experience -->
<!---
Looking back, the seminar was a lot of fun and -- from our perspective -- successful.
Especially considering that it was an experiment.
Everyone was highly motivated and we got great feedback from the students that they liked the format.
For the students it was a more work than a traditional seminar.
But in the end, our hope is that their effort will pay off for them as well, not only because of their increased visibility.
It was also more work for us supervisors.
But the extra effort was worth it, since limitations of interpretability are relevant for our research.
For me the seminar was an inspiration.
The students had new ideas and new perspectives to approach the limitations of interpretable machine learning.
-->

<!-- Technical setup -->
## Technical Setup {-}

The book chapters are written in the Markdown language.
The simulations, data examples and visualizations were created with R [@rlang].
To combine R-code and Markdown, we used rmarkdown.
The book was compiled with the bookdown package.
We collaborated using git and github.
For details, head over to the [book's repository](link/to/repo).



<!--chapter:end:index.Rmd-->

# Introduction

*Author: *

*Supervisor: *

## Intro About the Seminar Topic


## Outline of the Booklet


<!--chapter:end:00-introduction.Rmd-->

# Natural Variability by internal variability {#iv}

*Author: Author*

*Supervisor: Henri Funk*

*Suggested degree: Bachelor*

Natural variability refers to the inherent fluctuations in the climate system that occur without external forcings, such as changes in solar radiation, volcanic eruptions, or human-induced alterations of the Earth's atmosphere and surface. This variability can be due to a variety of factors, including atmospheric processes, ocean currents, the El Niño-Southern Oscillation (ENSO), and other dynamic components of the Earth system. Natural variability occurs across all time scales, from short-term (daily, seasonal) to long-term (decadal, centennial) fluctuations.

## Climate Model Ensemble

Climate models are sophisticated tools that simulate the interactions within the Earth's climate system. To understand and quantify natural variability, scientists use ensembles of climate model simulations. An ensemble consists of multiple runs of the same model, or different models, where each run has slightly different initial conditions or model parameters. This approach helps to capture the range of possible climate outcomes due to the inherent uncertainty and variability in the system.

## Internal Variability
Within the context of climate model ensembles, internal variability refers to the variations in climate that arise from the system's internal processes, independent of external forcings. This variability is a fundamental part of the climate system's dynamics and can lead to different outcomes even if the external conditions are the same.

@deser


<!--chapter:end:01-variability.Rmd-->

# Standard Precipitation Evapotranspiration Index {#spei}

*Author: Author*

*Supervisor: Henri Funk*

*Suggested degree: Bachelor*

## Definition

The SPEI is a statistical indicator that shows dry and wet periods on the basis of the climatic water balance (total precipitation minus total potential evaporation). It captures water deficits or surpluses on the land surface better than the Standardised Precipitation Index (SPI), which focuses solely on precipitation. The potential evaporation can be based on the so-called FAO grass evaporation, which takes into account radiation, air temperature, relative humidity and wind speed over a standard grass area. The time series is broken down into 12 monthly frequency sums, to each of which a cumulative log-logistic distribution is fitted. This is transformed into a corresponding cumulative standard normal distribution, the abscissa value of which - referred to as SPEI - allows simple assignment to probability classes.
@vicente

<!--chapter:end:02-spei.Rmd-->

# Compound events {#ce}

*Author: Author*

*Supervisor: Henri Funk*

*Suggested degree: Master*

In climate research, a compound event refers to the combination of multiple extreme weather or climate events occurring simultaneously or successively, leading to significant impacts on the environment, society, or economy. These events can be related either through meteorological factors (e.g., heatwaves and drought occurring together due to a prolonged high-pressure system) or through their impacts (e.g., heavy rainfall and storm surge combining to cause more severe flooding). The complexity of compound events lies in their interconnectedness and the way they can exacerbate each other's effects, often in a nonlinear manner, making them more challenging to predict and manage than individual extreme events.

## Example: Hot and Dry Events

Consider a scenario where a region experiences both extreme heat and an extended period of low precipitation simultaneously. In this example, the compound nature of the hot and dry events creates a cascade of impacts that are more severe than what would be expected if these events occurred independently. The interplay between the heat and drought amplifies the consequences, underscoring the importance of understanding and managing compound events in the context of climate change and variability.
@zscheischler


<!--chapter:end:03-compounds.Rmd-->

# Assymetric bivariate Copulas for compounds {#ac}

*Author: Author*

*Supervisor: Henri Funk*

*Suggested degree: Master*

In compounds, particularly in environmental and hydrological contexts, bivariate relationships can be used to study how two different environmental factors, such as temperature and precipitation, interact with each other. These relationships are critical for modeling the joint behavior of these variables, which can be essential for predicting weather events, designing structures, or managing natural resources.

In the study of environmental and hydrological systems, recognizing and accurately modeling the asymmetry in bivariate relationships is crucial for making reliable predictions and informed decisions. Techniques such as copulas, are often used in this context to model and analyze these complex and dependencies effectively. To model the asymmetry in dependencies, Archimax could be used.

@charpentier
@bacigal

<!--chapter:end:04-archimax.Rmd-->

# Teleconnections North Atlantic Oscillation {#nao}

*Author: Author*

*Supervisor: Henri Funk*

*Suggested degree: Bachelor/Master*

The concept of teleconnections in climate science refers to climate anomalies or patterns that are related across large distances, often thousands of kilometers apart. Teleconnections represent atmospheric interactions that link weather and climate conditions in one region of the globe with those in another, often through the movement and behavior of large-scale atmospheric wave patterns. These connections are crucial for understanding regional climate variations, predicting weather patterns, and assessing climate change impacts.

## North Atlantic Oscillation (NAO) and Its Implications

The North Atlantic Oscillation (NAO) is a prime example of a teleconnection pattern, characterized by variations in the difference of atmospheric pressure at sea level between the Icelandic Low and the Azores High. The NAO influences weather and climate conditions across the North Atlantic region and beyond, affecting winter temperatures and storm tracks.

## Implications for Our Climate

Understanding the NAO and other teleconnection patterns is essential for accurate weather forecasting, climate prediction, and the development of strategies to mitigate the impacts of climate variability and change. Researchers use observations, climate models, and paleoclimate reconstructions to study these patterns, aiming to improve our ability to predict their future behavior under different climate change scenarios.
@hurrell

<!--chapter:end:05-nao.Rmd-->

# Low Flow Events {#lfe}

*Author: Author*

*Supervisor: Henri Funk*

*Suggested degree: Bachelor*

Low flow events in hydrology refer to periods when water flow in rivers, streams, or creeks falls below a critical threshold, often leading to various ecological, economic, and social impacts. These events can result from prolonged periods of below-average precipitation, increased water usage, or changes in land use that affect the hydrological cycle. Low flow conditions can compromise water quality, reduce water availability for agricultural, industrial, and domestic uses, and disrupt aquatic ecosystems.

## Concept of Extremeness by Return Periods

The "extremeness" of hydrological events, including low flow conditions, is often characterized by return periods. A return period, also known as a recurrence interval, is a statistical measure used to estimate the frequency at which a certain intensity of an event (e.g., flow rate, rainfall amount) is likely to be equaled or exceeded. It is typically expressed in years.

- **Definition**: The return period is calculated based on historical data and is intended to provide an estimate of the likelihood of experiencing an event of a certain magnitude or greater within any given year. For example, a 100-year return period for a low flow event means that, on average, the flow rate is expected to fall to that level or lower once every 100 years. This does not imply the events will occur exactly 100 years apart but rather conveys a 1% chance of occurrence in any given year.
@du

<!--chapter:end:06-lowflow.Rmd-->

# Statistical streamflow modelling {#sm}

*Author: Author*

*Supervisor: Henri Funk*

*Suggested degree: Master*

Hydrological models are tools used to understand, simulate, and predict the movement, distribution, and quality of water within natural and engineered water systems. These models help in water resource management, flood forecasting, environmental protection, and understanding the impact of climate change on water resources.

There are mainly two types of hydrological models: physical and statistical.

## Physical Hydrological Models

Physical hydrological models are based on the physical processes that occur within the hydrological cycle. They use mathematical representations of these processes to simulate the behavior of water in the environment. Physical models consider processes such as precipitation, evaporation, infiltration, surface runoff, and groundwater flow. These models are often more complex and require detailed data about the terrain, soil properties, vegetation, and meteorological conditions.

## Statistical Hydrological Models

Statistical hydrological models use historical data to identify patterns and relationships between different hydrological variables. These models apply statistical methods to predict future water conditions based on past observations. They do not necessarily simulate the physical processes but rather rely on the statistical properties of the data, such as trends, cycles, and variability. Statistical models are often used when the physical processes are too complex to model directly or when there is insufficient data to build a detailed physical model.

@sabzipour

<!--chapter:end:07-hydroLSTM.Rmd-->

# The Lancet Report 2023    {#he1}

*Author: Author*

*Supervisor: Helmut Kuechenhoff*

*Suggested degree: Bachelor* 

In yearly reports one of the leading scientific journals in health research 
gives an overview on the state of the art knowledge on the effect of climate 
change on health (@romanello).  The following aspects should be treated: 

#. Main results and message 
#.  Communication of results 
#. Arguments based on statistical methods 


<!--chapter:end:08-lancet.Rmd-->

---
output:
  latex_engine: pdflatex
  keep_tex: true
  pdf_document: default
  word_document: default
  html_document: default
---

```{r message=FALSE, warning=FALSE, include=FALSE}
library(bookdown)
library(svglite)
```

# Epidemiologic studies on the heat effects {#he2}

*Author: Oussama Mesbah*

*Supervisor: Helmut Kuechenhoff*

*Degree: Bachelor*

## Introduction

This chapter covers epidemiologic studies on the effects of heat on health. The aim is to discuss the excess mortality attributed to heat and cold based on recent studies by Masselot et al. (2023) [@masselot2023] and the joint effects of air pollution and heat on mortality as examined by Stafoggia et al. (2023) [@stafoggia2023].

## Excess Mortality Attributed to Heat and Cold

### Background for Study

Masselot et al. (2023) conducted a comprehensive study on the mortality impacts of non-optimal ambient temperatures. Both heat and cold are established risk factors, and the study aims to provide a robust assessment of these impacts. Previous research has predominantly focused on country-specific data or omitted significant factors.

### Added Value

This study offers several key advancements:

First, it has unprecedented city coverage, examining 854 urban areas across Europe. This extensive coverage offers a more comprehensive assessment compared to prior studies that focused on specific countries or regions. Furthermore, the study balances its focus on both heat and cold impacts, providing a complete picture of temperature-related health risks. It explicitly models age-specific risks and provides age-standardized mortality rates, which allow for fair comparisons across different locations with varying demographic compositions. This is particularly important as it highlights the heightened vulnerability of the elderly population.

Compared to Zhao et al. (2021) [@zhao2021global] , Masselot et al. (2023) focuses on a more detailed regional scope by examining 854 cities in Europe, while Zhao's study covered 750 locations globally. Both studies cover the same period from 2000 to 2019, allowing for direct comparison. Masselot's study utilizes higher resolution ERA5-Land reanalysis data, which may provide a more accurate exposure assessment, especially for smaller cities. Additionally, the inclusion of age-specific mortality data in Masselot's study allows for a more nuanced analysis of age-related vulnerability to temperature extremes. Here is a table summarizing the key differences between the two studies:


| Feature                       | Zhao et al. (2021)            | Masselot et al. (2023)               | Added Benefit of Masselot et al. (2023)                       |
|-------------------------------|-------------------------------|--------------------------------------|---------------------------------------------------------------|
| **Geographical Scope**        | Global (750 locations in 43 countries) | Europe (854 cities in 30 countries)   | More detailed focus on a specific region.                      |
| **Temporal Scope**            | 2000-2019                     | 2000-2019                            | Allows direct comparison of results.                           |
| **Exposure Data**             | Gridded temperature data (0.5° x 0.5°) | ERA5-Land reanalysis data (9 km resolution) | Higher resolution data provides more accurate exposure assessment. |
| **Outcome Data**              | Daily all-cause or non-external mortality counts | Daily all-cause or non-accidental mortality counts, with age-specific data for a subset of cities | Inclusion of age-specific mortality data allows for nuanced analysis. |
| **Modeling Framework**        | Three-stage: time-series regression, meta-regression, prediction | Three-stage: quasi-Poisson regression with DLNM, meta-regression with PLS, kriging interpolation | Incorporation of PLS for dimensionality reduction and kriging for spatial interpolation. |
| **Vulnerability Assessment**  | Limited to 5 meta-predictors | Includes 22 city-specific characteristics | Comprehensive assessment of vulnerability factors. |
| **Age-Specific Analysis**     | Not explicitly mentioned      | Provides age-specific relative risks and age-standardized mortality rates | More targeted understanding of vulnerable age groups.         |



### Limitations

The study also has limitations. The extrapolation of exposure-response functions (ERFs) across European cities may introduce bias, especially in less-covered areas like Eastern Europe. Additionally, reliable ERFs could not be estimated for people younger than 20 years due to low death counts, limiting the analysis for younger populations.

### Findings

The study by Masselot et al. (2023) provides a detailed analysis of the impact of non-optimal temperatures on mortality across Europe. The study found an annual excess of 203,620 deaths due to cold and 20,173 deaths due to heat. Age-standardized excess death rates were calculated at 129 per 100,000 person-years for cold and 13 per 100,000 person-years for heat.

#### Regional Variations

The results indicated significant variations across different regions of Europe. The highest mortality effects were observed in eastern European cities, where the infrastructure and healthcare systems might be less equipped to handle extreme temperatures compared to their western counterparts.

![Relative Risk at Different Temperatures and Ages](work/09-epidemiologic/figures/Bild1.png) 

#### Age-Related Vulnerability

The study highlighted that vulnerability increases with age, particularly for individuals aged 85 and above. The pooled overall cumulative exposure-response relationship was predicted at several ages: 20-44 years, 45-64 years, 65-74 years, 75-84 years, and 85+ years. This data provides crucial insights into how different age groups respond to temperature extremes.

![Relative Risk for Cold and Heat in Capital Cities](work/09-epidemiologic/figures/Bild2.png)  

Several key terms are defined in the study to better understand the results:

- **Exposure-Response Function (ERF):** This describes the relationship between exposure levels (temperature) and mortality risk.
- **Minimum Mortality Temperature (MMT):** The specific temperature at which mortality risk is lowest.
- **Minimum Mortality Percentile (MMP):** The temperature range where the population is generally healthiest.
- **Relative Risk (RR):** The ratio of mortality risk at a specific temperature compared to the MMT.

The study also analyzed the relative risks for cold and heat in capital cities across five age groups. The findings showed that excess mortality is generally lower in Western Europe compared to Northern and Southern Europe.

#### Geographical Gradient

One notable finding is the north-south gradient in the Minimum Mortality Percentile (MMP) and Minimum Mortality Temperature (MMT). As temperatures get hotter, the MMP decreases and the MMT increases, indicating that populations in hotter climates have adapted to higher temperatures as their baseline for optimal health.

| ![](work/09-epidemiologic/figures/Bild5.png) | ![](work/09-epidemiologic/figures/Bild6.png) |
|:--------------------------------------------:|:--------------------------------------------:|
| Minimum Mortality Temperature Across Europe      | Minimum Mortality Percentile Across Europe  |


 
The study's maps illustrate these gradients and the spatial distribution of temperature-related mortality risks. For example, cold-related mortality shows higher excess death rates in Eastern Europe, while heat-related mortality is more prominent in Southern Europe.


| ![](work/09-epidemiologic/figures/Bild7.png) | ![](work/09-epidemiologic/figures/Bild8.png) |
|:--------------------------------------------:|:--------------------------------------------:|
| Cold-Related Standardized Excess Death Rate      | Heat-Related Standardized Excess Death Rate  


These visualizations provide a comprehensive overview of the temperature-mortality relationship across Europe, highlighting regions and populations that are most vulnerable to extreme temperatures.

### Conclusion

The detailed findings from this study underscore the importance of tailored public health strategies that consider regional and age-specific vulnerabilities. By understanding the nuanced impacts of heat and cold across different European cities and demographic groups, policymakers can better prepare for and mitigate the adverse health effects of extreme temperatures.


### Data and Framework

The study utilized data from 854 urban audit cities from Eurostat and 232 cities from the Multicountry Multi-city (MCC) Collaborative Research Network. City-specific meta-predictors included total population, proportion of population above 65 years, population density, GDP per capita, and environmental factors like PM2.5 concentration and annual temperature range.

Dimensionality reduction is achieved using PLS regression to create composite indices of vulnerability. This method helps in capturing geographical variability in risks while avoiding overfitting. PLS regression is used for dimensionality reduction and predictive modeling, particularly useful when dealing with a large number of highly correlated predictors. The Akaike Information Criterion (AIC) helps select the optimal number of PLS components by balancing goodness of fit with model complexity.

### Modelling Framework

The modelling framework includes several stages:
1. **City-Specific Risk Estimation:** Using a quasi-Poisson regression model with a distributed lag non-linear model (DLNM).
2. **Meta-Regression and Spatial Interpolation:** Pooling city-specific estimates into a multivariate multilevel meta-regression model.
3. **Prediction and Impact Assessment:** Predicting ERFs for all 854 cities and estimating excess deaths.

Here is a Flowchart of the Modelling Framework:

![Modelling Framework Flowchart](work/09-epidemiologic/figures/Bild11.png)

### Partial Least Squares (PLS) Regression

Partial Least Squares (PLS) regression is a statistical method used for dimensionality reduction and predictive modeling. It is particularly useful when dealing with a large number of highly correlated predictors. The goal of PLS is to find the fundamental relations between two matrices (predictors and responses) by projecting the predictors to a new space.

In the context of epidemiologic studies on the heat effects, PLS regression helps in creating composite indices of vulnerability from city-specific characteristics. This method captures geographical variability in risks while avoiding overfitting.

#### PLS Algorithm

The PLS algorithm iteratively constructs new predictor variables (latent variables) that are linear combinations of the original predictor variables. These new variables are uncorrelated and explain the maximum possible variance in the response variables.

The steps involved in PLS are as follows:

1. **Standardize the predictors and responses**:
   \[
   \mathbf{X} = \frac{\mathbf{X} - \mathbf{\bar{X}}}{\mathbf{S_X}}
   \]
   \[
   \mathbf{Y} = \frac{\mathbf{Y} - \mathbf{\bar{Y}}}{\mathbf{S_Y}}
   \]

2. **Calculate the covariance matrix**:
   \[
   \mathbf{C} = \mathbf{X}^T \mathbf{Y}
   \]

3. **Extract the first latent variable**:
   \[
   \mathbf{w}_1 = \arg \max \left( \mathbf{w}^T \mathbf{C} \mathbf{w} \right)
   \]
   Subject to \(\|\mathbf{w}\| = 1\).

4. **Compute the scores for the first latent variable**:
   \[
   \mathbf{t}_1 = \mathbf{X} \mathbf{w}_1
   \]
   \[
   \mathbf{u}_1 = \mathbf{Y} \mathbf{q}_1
   \]
   Where \(\mathbf{q}_1 = \mathbf{C} \mathbf{w}_1\).

5. **Deflate the matrices**:
   \[
   \mathbf{X}_1 = \mathbf{X} - \mathbf{t}_1 \mathbf{p}_1^T
   \]
   \[
   \mathbf{Y}_1 = \mathbf{Y} - \mathbf{t}_1 \mathbf{c}_1^T
   \]
   Where \(\mathbf{p}_1 = \mathbf{X}^T \mathbf{t}_1 / (\mathbf{t}_1^T \mathbf{t}_1)\) and \(\mathbf{c}_1 = \mathbf{Y}^T \mathbf{t}_1 / (\mathbf{t}_1^T \mathbf{t}_1)\).

6. **Repeat steps 2-5** for the remaining components.

#### Geometric View

Geometrically, PLS regression can be viewed as projecting the original predictor space \(\mathbf{X}\) into a lower-dimensional latent space while maximizing the covariance between the predictors and the responses \(\mathbf{Y}\).

The projections are chosen to ensure that each successive pair of latent variables captures the maximum possible covariance between \(\mathbf{X}\) and \(\mathbf{Y}\).


![Geometric View of PLS](work/09-epidemiologic/figures/Bild9.png)

### Akaike Information Criterion (AIC) and Model Selection

AIC is used to select the optimal number of PLS components. It balances the goodness of fit with the complexity of the model. Lower AIC values indicate a better model fit, accounting for the number of parameters used.

The formula for AIC is:
\[
\text{AIC} = 2k - 2\ln(\hat{L})
\]
where \(k\) is the number of parameters in the model and \(\hat{L}\) is the maximized value of the likelihood function for the estimated model.

The plot below shows the AIC values for different numbers of PLS components. The optimal number of components is indicated by the lowest AIC value, which in this case is four components. This balance ensures that the model is neither overfitted nor underfitted, providing a parsimonious and explanatory model for predicting temperature-related mortality risks.

![AIC Values for Different Numbers of PLS Components](work/09-epidemiologic/figures/Bild12.png) \footnote{Source: Excess mortality attributed to heat and cold}

### Application in the Study

In the epidemiologic study on the heat effects, PLS regression was performed by reducing the dimensionality of city-specific characteristics into a few uncorrelated components. These components were then used in the meta-regression model to predict temperature-related mortality risks across different cities and age groups.

The PLS regression allowed the study to incorporate a large number of variables, such as demographic and environmental factors, without overfitting the model. This approach provided a robust framework for assessing the vulnerability of different urban areas to heat and cold effects.

By selecting the optimal number of components through AIC, the study ensured that the models were both parsimonious and explanatory, leading to reliable predictions of excess mortality due to temperature extremes.


### Application in the Study

In the epidemiologic study on the heat effects, PLS regression was performed by reducing the dimensionality of city-specific characteristics into a few uncorrelated components. These components were then used in the meta-regression model to predict temperature-related mortality risks across different cities and age groups.

The PLS regression allowed the study to incorporate a large number of variables, such as demographic and environmental factors, without overfitting the model. This approach provided a robust framework for assessing the vulnerability of different urban areas to heat and cold effects.

By selecting the optimal number of components through AIC, the study ensured that the models were both parsimonious and explanatory, leading to reliable predictions of excess mortality due to temperature extremes.



### Model

The model used to estimate the main effect of air temperature on mortality employs a quasi-Poisson regression framework. This framework is suitable for count data, such as daily death counts, and can handle overdispersion commonly observed in such data.[@engelhardt2013]

#### Quasi-Poisson Regression Model

The primary model is formulated as follows:
\[ \text{log}(\mu_i) = \alpha + \sum_{j=1}^{p} \beta_j x_{ij} + \gamma z_i \]
where:
- \(\mu_i\) is the expected daily death count for day \(i\),
- \(\alpha\) is the intercept,
- \(x_{ij}\) represents the predictor variables (e.g., temperature, humidity, etc.) for day \(i\) and predictor \(j\),
- \(\beta_j\) are the regression coefficients for the predictor variables,
- \(z_i\) includes other confounders such as air pollution levels, day of the week, and long-term trends,
- \(\gamma\) represents the coefficients for the confounders.

The quasi-Poisson model is an extension of the Poisson model, which allows for overdispersion by adding a dispersion parameter \(\theta\):
\[ \text{Var}(Y_i) = \theta \mu_i \]

### Assessing Overdispersion

In epidemiological studies involving count data, such as daily mortality counts, it is essential to assess for overdispersion. Overdispersion occurs when the observed variance in the count data is greater than what is expected under a standard Poisson model. This can lead to underestimated standard errors and consequently overconfident statistical inferences.

#### Overdispersion in Poisson Models

The Poisson regression model assumes that the mean and variance of the count data are equal (\(\mu = \sigma^2\)). However, in real-world data, this assumption often does not hold, especially in the presence of unobserved heterogeneity or clustering effects.

To assess for overdispersion, we can use the following diagnostic measures:

1. **Dispersion Parameter (\(\theta\))**:
   The dispersion parameter is defined as the ratio of the Pearson chi-square statistic to the degrees of freedom:
   \[
   \theta = \frac{\sum_{i=1}^{n} (y_i - \hat{\mu}_i)^2 / \hat{\mu}_i}{n - p}
   \]
   where \(y_i\) is the observed count, \(\hat{\mu}_i\) is the predicted count, \(n\) is the number of observations, and \(p\) is the number of parameters estimated in the model. A \(\theta\) value greater than 1 indicates overdispersion.

2. **Pearson Chi-Square Statistic**:
   The Pearson chi-square statistic is calculated as:
   \[
   X^2 = \sum_{i=1}^{n} \frac{(y_i - \hat{\mu}_i)^2}{\hat{\mu}_i}
   \]
   This statistic is compared against the chi-square distribution with \(n - p\) degrees of freedom. A significantly large value indicates overdispersion.

3. **Residual Deviance**:
   The residual deviance should be approximately equal to the degrees of freedom under the Poisson assumption. A much larger residual deviance suggests overdispersion:
   \[
   \text{Deviance} = 2 \sum_{i=1}^{n} \left[ y_i \log\left(\frac{y_i}{\hat{\mu}_i}\right) - (y_i - \hat{\mu}_i) \right]
   \]

#### Addressing Overdispersion

When overdispersion is detected, alternative modeling approaches should be considered to obtain valid statistical inferences:

1. **Quasi-Poisson Model**:
   The quasi-Poisson model adjusts for overdispersion by introducing a dispersion parameter:
   \[
   \text{Var}(Y_i) = \theta \mu_i
   \]
   This model provides robust standard errors that account for overdispersion.

2. **Negative Binomial Model**:
   The negative binomial model is another approach that introduces an additional parameter to model the overdispersion. The variance is modeled as:
   \[
   \text{Var}(Y_i) = \mu_i + \alpha \mu_i^2
   \]
   where \(\alpha\) is the overdispersion parameter. This model is particularly useful when the overdispersion increases with the mean.

3. **Generalized Estimating Equations (GEE)**:
   GEEs provide a flexible approach to handle correlated and overdispersed data by specifying a working correlation structure. They produce robust standard errors that are valid even if the specified correlation structure is incorrect.

#### Application in the Study

In the context of the study on the effects of temperature on mortality, assessing overdispersion is crucial for ensuring the validity of the model estimates. The quasi-Poisson regression model was chosen for this study due to its ability to handle overdispersed data effectively. The dispersion parameter was estimated, and diagnostic checks were performed to confirm the presence of overdispersion.

By using the quasi-Poisson model, the study obtained robust standard errors and more reliable estimates of the temperature-mortality relationship. This approach ensured that the findings were not biased by underestimated variability, leading to more accurate and confident public health recommendations.

#### Quasi-Poisson Regression Model

The Quasi-Poisson regression model adjusts for overdispersion in count data like mortality. The model is represented as:
\[ \text{log}(\mu_i) = \alpha + \sum_{j=1}^{p} \beta_j x_{ij} + \gamma z_i \]
where \(\mu_i\) is the expected daily death count, \(\alpha\) is the intercept, \(x_{ij}\) represents the predictor variables (e.g., temperature), \(\beta_j\) are the regression coefficients, and \(z_i\) includes other confounders like air pollution levels.

#### Distributed Lag Non-Linear Model (DLNM)

To capture the delayed and non-linear effects of temperature on mortality, a Distributed Lag Non-Linear Model (DLNM) is used [@gasparrini2010distributed]. The DLNM incorporates the lagged effects of temperature, recognizing that exposure to extreme temperatures can affect mortality over several days. The model can be expressed as:
\[ \text{log}(\mu_i) = \alpha + f(\text{temperature}_{i-l}) + \sum_{k} \beta_k z_{ik} \]
where \(f(\text{temperature}_{i-l})\) represents a smooth function of temperature over lag \(l\) days, and the other terms are as previously defined.

The smooth function \(f\) is typically modeled using splines, allowing for flexible non-linear relationships:
\[ f(\text{temperature}_{i-l}) = \sum_{m=1}^{M} s_m(\text{temperature}_{i-l}) \]
where \(s_m\) are the spline basis functions and \(M\) is the number of basis functions.

#### Meta-Analysis

In the second stage, the results from individual city-specific models are pooled using a multilevel random-effects meta-analysis. This approach combines the estimates from different cities while accounting for between-city variability. The random-effects model is given by:
\[ \hat{\theta}_j = \theta + u_j + \epsilon_j \]
where:
- \(\hat{\theta}_j\) is the estimated effect size for city \(j\),
- \(\theta\) is the overall effect size,
- \(u_j\) is the random effect for city \(j\), assumed to follow a normal distribution with mean 0 and variance \(\tau^2\),
- \(\epsilon_j\) is the within-city error term.

The overall effect size \(\theta\) is estimated by weighting the city-specific estimates based on their variances:
\[ \hat{\theta} = \frac{\sum_{j=1}^{J} w_j \hat{\theta}_j}{\sum_{j=1}^{J} w_j} \]
where \(w_j = \frac{1}{\text{Var}(\hat{\theta}_j) + \tau^2}\) are the weights.

#### Incorporating Age-Specific Effects

The model also considers age-specific effects by stratifying the analysis by different age groups. Age-stratified models help in understanding the differential impact of temperature on various age cohorts. The age-specific model is given by:
\[ \text{log}(\mu_{i,a}) = \alpha_a + \sum_{j=1}^{p} \beta_{j,a} x_{ij} + \gamma_a z_i \]
where \(\mu_{i,a}\) is the expected daily death count for age group \(a\), and the other terms are as previously defined but specific to age group \(a\).

### Findings and Interpretation

The combination of quasi-Poisson regression, DLNM, and multilevel meta-analysis provides a robust framework for estimating the temperature-mortality relationship. The pooled estimates across cities indicate the overall effect of temperature on mortality while accounting for local variations and confounders.

The findings from the model underscore the significant impact of temperature extremes on mortality, with cold temperatures generally showing a larger effect than heat in many European cities. The results highlight the importance of considering both immediate and lagged effects of temperature, as well as the need for city-specific and age-specific analyses to inform targeted public health interventions.

By employing these advanced statistical techniques, the study provides comprehensive and nuanced insights into the public health implications of temperature variability and extreme weather events.


## Joint Effect of Heat and Air Pollution on Mortality

### Findings

The study by Stafoggia et al. (2023) examined the combined effects of high temperatures and air pollution on mortality. The analysis involved data from 620 cities in 36 countries, providing a comprehensive overview of the joint impact of these environmental stressors. The findings indicated that both high temperatures and air pollution independently increase mortality rates. However, their combined effect is significantly more pronounced. Specifically, a temperature increase from the 75th to the 99th percentile was associated with an 8.9% rise in mortality.

### Model

To estimate the joint effect of air temperature and air pollution on mortality, a two-stage modeling approach was employed.

#### First Stage: City-Specific Models

In the first stage, city-specific models were developed using a quasi-Poisson regression framework. The model is expressed as:

\[ \text{log}(\mu_{i}) = \alpha + \beta_1 \text{Temp}_{i} + \beta_2 \text{PM}_{i} + \beta_3 (\text{Temp}_{i} \times \text{PM}_{i}) + \sum_{j=1}^{p} \gamma_j z_{ij} \]

where:
- \(\mu_{i}\) is the expected daily death count for day \(i\),
- \(\alpha\) is the intercept,
- \(\text{Temp}_{i}\) represents the temperature on day \(i\),
- \(\text{PM}_{i}\) represents the air pollution level (e.g., PM2.5 concentration) on day \(i\),
- \(\text{Temp}_{i} \times \text{PM}_{i}\) is the interaction term between temperature and air pollution,
- \(\beta_1, \beta_2,\) and \(\beta_3\) are the coefficients for temperature, air pollution, and their interaction, respectively,
- \(z_{ij}\) represents other covariates (e.g., day of the week, long-term trends),
- \(\gamma_j\) are the coefficients for the covariates.

The interaction term (\(\beta_3\)) captures the joint effect of temperature and air pollution on mortality, indicating how the combined exposure amplifies the risk.

#### Distributed Lag Non-Linear Model (DLNM)

The DLNM was used to account for the delayed effects of temperature and air pollution.  The model is formulated as:

\[ \text{log}(\mu_{i}) = \alpha + f(\text{Temp}_{i-l}, \text{PM}_{i-l}) + \sum_{k=1}^{p} \gamma_k z_{ik} \]

where:
- \(f(\text{Temp}_{i-l}, \text{PM}_{i-l})\) is a bivariate smooth function of temperature and air pollution over lag \(l\) days,
- \(z_{ik}\) are other covariates.

This model allows for flexible modeling of the non-linear and lagged relationships between temperature, air pollution, and mortality.

#### Second Stage: Meta-Analysis

In the second stage, the city-specific estimates were pooled using a multilevel random-effects meta-analysis. This approach combines the results from different cities, accounting for between-city heterogeneity.

The random-effects model is given by:

\[ \hat{\theta}_j = \theta + u_j + \epsilon_j \]

where:
- \(\hat{\theta}_j\) is the estimated effect size for city \(j\),
- \(\theta\) is the overall effect size,
- \(u_j\) is the random effect for city \(j\), assumed to follow a normal distribution with mean 0 and variance \(\tau^2\),
- \(\epsilon_j\) is the within-city error term.

The overall effect size \(\theta\) is estimated by weighting the city-specific estimates based on their variances:

\[ \hat{\theta} = \frac{\sum_{j=1}^{J} w_j \hat{\theta}_j}{\sum_{j=1}^{J} w_j} \]

where \(w_j = \frac{1}{\text{Var}(\hat{\theta}_j) + \tau^2}\) are the weights.

### Comparison of Findings: Masselot et al. (2023) vs. Stafoggia et al. (2023)

Both studies highlight the significant impact of temperature on mortality, but they approach the analysis from different angles and include different additional variables.

#### Masselot et al. (2023) [@masselot2023]

- **Focus**: Assessed the impact of non-optimal temperatures (both heat and cold) on mortality across Europe.
- **Model**: Used a quasi-Poisson regression with DLNM for temperature effects, and meta-regression for pooling city-specific estimates.
- **Key Findings**: Higher mortality rates associated with cold temperatures compared to heat. Significant regional variations with Eastern Europe showing higher vulnerability. Age-specific analysis revealed increased vulnerability in older age groups.
- **Data**: Included 854 cities across Europe.

#### Stafoggia et al. (2023) [@stafoggia2023]

- **Focus**: Examined the joint effect of high temperatures and air pollution on mortality.
- **Model**: Used a quasi-Poisson regression with an interaction term for temperature and air pollution, combined with DLNM for lagged effects. Pooled city-specific estimates using multilevel random-effects meta-analysis.
- **Key Findings**: Both high temperatures and air pollution independently increase mortality, but their combined effect is more pronounced. The interaction between temperature and air pollution significantly amplifies mortality risk.
- **Data**: Included 620 cities in 36 countries.

#### Comparison of Temperature Effects

- **Temperature Alone**:
  - Both studies confirm that temperature extremes (both high and low) significantly affect mortality rates.
  - Masselot et al. focused on a broad spectrum of temperature effects, including both heat and cold.
  - Stafoggia et al. specifically addressed the interaction between heat and air pollution.

- **Combined Effects**:
  - The interaction term in Stafoggia et al.'s model revealed a synergistic effect, where the combined exposure to heat and air pollution resulted in a higher mortality risk than the sum of their individual effects.
  - This aspect was not covered in Masselot et al.'s study, which did not include air pollution as a variable.

- **Methodological Differences**:
  - Masselot et al. used a wider range of temperatures and focused on a comprehensive assessment of cold and heat effects, including age-specific vulnerabilities.
  - Stafoggia et al. provided insights into the compounded effects of environmental stressors, emphasizing the need to consider multiple factors in public health policies.

### Conclusion

Both studies contribute valuable insights into the public health impacts of temperature extremes. Masselot et al. (2023) [@masselot2023] highlighted the broader spectrum of temperature effects across Europe, while Stafoggia et al. (2023) [@stafoggia2023] provided a nuanced understanding of the combined effects of heat and air pollution. Together, these studies underscore the importance of considering both direct and interactive effects of environmental stressors in epidemiological research and public health planning.

<!--chapter:end:09-epidemiologic.Rmd-->

# Controverial issue : heat and humidity    {#he3}

*Author: Author*

*Supervisor: Helmut Kuechenhoff*

*Degree: Master*   

In a paper the possible interaction of humidity and  heat is discussed (@baldwin). 
In particular, possible pitfalls in in epidemiological studies are highlighted.
Statistical and methodological aspects should be highlighted. 


  


<!--chapter:end:10-heatandhum.Rmd-->

# Risk Projections   {#he4}

*Author: Author*

*Supervisor: Helmut Kuechenhoff*

*Degree: Bachelor*   

In many studies, the effects of climate change in the future is discussed. 
In a tutorial paper (@vicedo) , statistical methods for modelling future risks due to climate devlopement are presented. In a two other papers paper the future effects of ozone and heat  related are presented (@domingo,@chen).   



<!--chapter:end:11-projections.Rmd-->

# Open issue: Future health risks of climate change {#he5}

*Author: Author*

*Supervisor: Helmut Kuechenhoff*

*Degree: Bachelor Master  *   

Further literature should be checked. 
Possible starting point : 
Report of the IPCC- panel (@IPCC 3000 pages) 



<!--chapter:end:12-future.Rmd-->

# Open issue

*Author: Author*

*Supervisor: Helmut Kuechenhoff*

*Degree: Bachelor Master  *   

Further aspects of climate change 

Economic, political aspects, communication 

@campbell @sun 

<!--chapter:end:13-open.Rmd-->

# Acknowledgements

The most important contributions are from the students themselves.
The success of such projects highly depends on the students.
And this book is a success, so thanks a lot to all the authors!
The other important role is the supervisor.
Thanks to all the supervisors who participated!
Special thanks to [SUPERVISING PROFESSOR](https://www.stablab.stat.uni-muenchen.de/personen/leitung/kuechenhoff1/index.html) who enabled us to conduct the seminar in such an experimental way, supported us and gave valuable feedback for the seminar structure.
Thanks a lot as well to the entire [Department of Statistics](https://www.statistik.uni-muenchen.de/) and the [LMU Munich](http://www.en.uni-muenchen.de/index.html) for the infrastructure.

The authors of this work take full responsibilities for its content.



<!--chapter:end:98-acknowledgments.Rmd-->

`r if (knitr:::is_html_output()) '# References {-}'`

```{r include=FALSE}
# generate a BibTeX database automatically for some R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```


<!--chapter:end:99-references.Rmd-->

