---
output:
  pdf_document: default
  keep_tex: true
  latex_engine: pdflatex
  word_document: default
  html_document: default
---

*Author: Lennart Marx*

*Supervisor: Henri Funk*

*Degree: Master*

```{r message=FALSE, warning=FALSE, include=FALSE}
library(bookdown)
library(svglite)
```

# Statistical streamflow modelling {#sm}

## Abstract

This study evaluates and compares the performance of Long Short-Term Memory (LSTM) networks and Temporal Fusion Transformers (TFT) in forecasting streamflows up to seven days ahead, using historical streamflow data alongside precipitation and temperature as covariates. Data from the Regen River in Bavaria, Germany, was used. Preprocessing involved spatial averaging of meteorological data within defined catchments. Results indicate that while both models face challenges in predicting extreme values, TFT maintains more consistent accuracy over longer forecast horizons while pure LSTM model's predictions decline sharply in performance with increasing lead time. The study highlights the importance of future known meteorological variables in achieving accurate predictions and suggests avenues for future research to enhance model robustness and data preprocessing techniques.

## Introduction

Accurate streamflow prediction is beneficial for a variety of applications, including flood forecasting or hydroelectric power generation. Timely and precise flood predictions can mitigate the impact of flood events, protecting lives and property. Similarly, reliable streamflow forecasts are essential for optimizing the operation of hydroelectric plants, ensuring efficient energy production and contributing to the stability of power grids.

Streamflow refers to the flow of water in rivers and streams, originating from precipitation, melting snow, and groundwater discharge. It is typically measured in cubic meters per second (m³/s).

In recent years, machine learning techniques have emerged as powerful tools for forecasting in various domains, including hydrology. Traditional hydrological models often rely on extensive datasets and intricate physical parameters, which can be challenging to obtain and process. In contrast, machine learning models such as Long Short-Term Memory (LSTM) networks offer an alternative approach by learning patterns directly from historical data. These models have shown promise in capturing the complex, nonlinear relationships inherent in hydrological systems.

This study aims to evaluate and compare the performance of LSTM and TFT models in forecasting streamflows up to seven days ahead. By incorporating precipitation and temperature as future known covariates alongside historical streamflow data, we seek to determine the effectiveness of these models in predicting streamflows.

Understanding the strengths and limitations of these models is essential for their practical application. Flood events often involve extreme values that can be difficult to predict accurately, while hydroelectric generation requires consistent and reliable forecasts over varying time horizons. Thus, this research not only assesses the overall accuracy of LSTM and TFT models but also examines their performance in predicting extreme streamflow values and their resource dependency, providing insights into their feasibility for real-world applications.

## Data

### Preperation

The streamflow data is gathered from the bavarian hydrology authority (GKD). They provide freely accessible data on most water bodies in Bavaria @bayerisches_landesamt.Focusing on rivers of medium size where the entire length is flowing inside of the area of the state of Bavaria, the river Regen located north/east of the city of Regensburg was chosen as a bases for this study.The GKD posesses 21 streamflow gauging stations located at the Regen or any of its tributary rivers. For the Regen river, data between 2014 and 2024 was available with daily measurements on the streamflows including the coordinates of the the gauging station. This study focused on the 15207507 Marienthal gauging station that was located closest to the final outflow towards the Danube river. Utilizing the HydroRIVERS dataset, which contains the shapes of rivers all over the world, it was possible the acquire the shape of the Regen along with its geolocation as shown in figure \@ref(fig:1) @lehner.

```{r 1,fig.cap = 'Streamflow Gauging Stations that provide there measurements at the GKD along the Regen river', cache=FALSE, out.width="500px", fig.align="center", echo=FALSE, eval = TRUE}
knitr::include_graphics('work/07-hydroLSTM/images/regen_stations.png')
```

A catchment also known as a drainage basin or watershed, is an area of land where all precipitation collects and drains off into a common outlet, such as a river, bay, or other body of water. The boundaries of a catchment are defined by the highest points of land around the drainage area, often referred to as the drainage divide. These catchment areas will later be used to determine the variables for precipitation that are used to forecast the streamflows of the river. Taking advantage of the HydroBASINS dataset, that contains the shapes of basins all over the world in different resolutions @lehner. With the software QGIS a Geoinformation System (GIS) all basins were chosen that completely contain the river Regen Shape, which led to the 19 defined chatchments that can be seen figure \@ref(fig:2).

```{r 2, cache=FALSE, fig.cap = 'Catchments defined for the Regen river based on the HydroBASINS shapefile dataset',out.width="500px", fig.align="center", echo=FALSE, eval = TRUE}
knitr::include_graphics('work/07-hydroLSTM/images/river_catch.png')
```

The ERA5 reanalysis dataset, contains a plethora of meteorological data in a of rasterized form. Each data cell contains information of when it occured, where it occured in the form of longitude and latitude coordinates as well as the information on a meteorological variables @hersbach2023era5. As proposed in @sabzipour, for this study the variables 2 meter temperature and total precipitation were selected around the area of the Regen and for the past 10 years.

### Preprocessing

The measuring station 15207507 contained 1 missing value which was imputed by linear interpolation. As suggested in @sabzipour non centered moving average smoothing with a window size of 3 was applied to the streamflow data as can be seen in figure \@ref(fig:3).

```{r 3, cache=FALSE, out.width="500px",fig.cap = 'streamflows of the 15207507 Marienthal gauging stations before (blue) and after (orange) applying moving average smoothing', fig.align="center", echo=FALSE, eval = TRUE}
knitr::include_graphics('work/07-hydroLSTM/images/Moving_Average.png')
```

To combine the rasterized meteorological data from ERA5 with the streamflows of the Regen, it is necessary to only take precipitation into account that occurs within the defined catchments. To achieve this, a weighted average is calculated, where the weights are determined by the area size of the intersection between the raster cell of the meteorological data and the catchment. This can be seen in a schematic visualization in figure \@ref(fig:4).

```{r 4, cache=FALSE, out.width="500px",fig.cap = 'schematic visualization of the spatial averaging performed for temperature and total precipitation during data preprocessing', fig.align="center", echo=FALSE, eval = TRUE}
knitr::include_graphics('work/07-hydroLSTM/images/spatial_averaging.png')
```

$$ t2m(\text{catch}_i) = \frac{1}{A(\text{catch}_i)} \sum_{j \in \text{ERA5 raster}} \left( t2m(\text{cell}_j) \cdot A(\text{catch}_i \cap \text{cell}_j) \right) \tag{1} $$

Since the catchments are geospatially close to each other the different values in the catchments provided little variance. (IMAGE CORRELATION Table). To reduce the feature space for both variables temperature and precipitation, the mean is taken over all catchments. Finally to reduce the noise in the features a moving average of window size 3 was applied.

## Models

### LSTM

The Long Short-Term Memory (LSTM) cell (figure \@ref(fig:5)) is a type of recurrent neural network (RNN) architecture designed to model temporal sequences and their long-range dependencies more accurately than traditional RNNs. LSTMs were introduced by Sepp Hochreiter and Jürgen Schmidhuber in 1997 to address the issue of vanishing and exploding gradients encountered in traditional RNNs, which struggle to maintain context over long sequences @hochreiter.

```{r 5, cache=FALSE, out.width="500px", fig.cap = 'visualization of a LSTM cell at time step t (image adapted from @lstm_image',fig.align="center", echo=FALSE, eval = TRUE}
knitr::include_graphics('work/07-hydroLSTM/images/LSTM.png')
```

1.  **Cell State (**$C_t$): The internal memory of the cell, which can carry information across many time steps.
2.  **Hidden State (**$h_t$): The output of the LSTM cell at a given time step, also serving as the input to the next cell.
3.  **Input Gate (**$i_t$): Controls how much of the new information from the current input is used to update the cell state.
4.  **Forget Gate (**$f_t$): Decides how much of the past cell state should be forgotten.
5.  **Output Gate (**$o_t$): Determines the output of the LSTM cell based on the cell state.

LSTMs are particularly well-suited for tasks that involve sequential data and temporal dependencies, such as:

1.  **Natural Language Processing (NLP)**:
    -   Language modeling
    -   Machine translation
    -   Speech recognition
2.  **Time Series Forecasting**:
    -   Stock price prediction
    -   Weather forecasting
    -   Anomaly detection

Streamflow forecasting involves predicting the flow of water in rivers and streams over time, which is inherently a time-series problem with temporal dependencies influenced by various factors such as rainfall, snowmelt, and upstream water management. When used in an encoder decoder architecture LSTM-cells can also incorporate Future known covariates such as weather forecasts. These specifications make LSTM-based architectures beneficial for modeling and forecasting streamflow data.

### Temporal Fusion Transformer

The Temporal Fusion Transformer (TFT) (figure \@ref(fig:6)) is a neural network architecture specifically designed for multi-horizon time series forecasting. It combines the strengths of both recurrent and attention-based models, offering an advanced approach to handling complex time series data. The TFT was introduced by Bryan Lim et al. in 2019, aiming to provide interpretability and accuracy for forecasting tasks @LIM20211748.

```{r 6, cache=FALSE, out.width="500px", fig.cap = 'model architecture for the Temporal Fusion Transformer (image from @LIM20211748', fig.align="center", echo=FALSE, eval = TRUE}
knitr::include_graphics('work/07-hydroLSTM/images/TFT.png')
```

A TFT consists of several key components:

1.  **Temporal Processing**:
    -   **Local Processing with LSTMs**: LSTMs are used to process local temporal dependencies within the time series.
    -   **Multi-Head Attention**: An attention mechanism to capture long-range dependencies across different time steps.
2.  **Variable Selection**:
    -   **Static Covariate Encoders**: Handle static features that do not change over time (e.g., location-specific data).
    -   **Temporal Covariate Encoders**: Manage time-varying features (e.g., weather data, past values of the time series).
3.  **Gating Mechanisms**:
    -   **Gated Residual Network (GRN)**: Ensures that only relevant information is passed through layers, improving the network's efficiency and interpretability.
    -   **Variable Selection Networks**: Dynamically select relevant variables at each time step to enhance model performance and interpretability.
4.  **Multi-Horizon Forecasting**:
    -   **Sequence-to-Sequence Framework**: Allows the TFT to generate forecasts for multiple future time steps simultaneously.
5.  **Interpretable Outputs**:
    -   **Attention Weights**: Provide insights into which time steps and variables the model is focusing on, aiding interpretability.

The Temporal Fusion Transformer represents an advancement in time series forecasting, offering both high accuracy and interpretability. Its ability to capture complex dependencies, dynamically select relevant features, and provide insights into the decision-making process makes it a useful tool for streamflow forecasting.

### Kling Gupta Efficiency

The Kling-Gupta Efficiency (KGE) is a statistical metric used to evaluate the performance of hydrological models by comparing simulated data to observed data. Developed by Gupta et al. in 2009, the KGE addresses limitations found in traditional metrics such as the Nash-Sutcliffe Efficiency (NSE). The KGE decomposes the evaluation of model performance into three distinct components, providing a more comprehensive assessment. These components are correlation, bias, and variability, which help in understanding different aspects of the model's accuracy.

The KGE is calculated using the following formula:

$$ \text{KGE} = 1 - \sqrt{(r - 1)^2 + (\alpha - 1)^2 + (\beta - 1)^2} \tag{2} $$

-   $r$ is the Pearson correlation coefficient between the simulated and observed values.

-   $\alpha$ is the bias ratio, defined as the ratio of the mean of the simulated values to the mean of the observed values.

-   $\beta$ is the standard deviation ratio, defined as the ratio of the standard deviation of the simulated values to the standard deviation of the observed values.

## Results

### Training Setup

The 2 different model architectures were trained using the historical streamflows as well as temperature and precipitation as covariates. Using an input sequence length of 364 days and an output lead time of up to 7 days. Temperature and precipitation can be used as future known values when considering weather forecasts. For example when trying to predict one step ahead forecast for the streamflow in addition to the past 364 days of precipitation values one can consider the precipitation forecast for the next day to get the best predictions possible.

The LSTM Model is run in an encoder decoder architecture, were the past 364 days are the input for an LSTM cell which returns a hidden state and an output as the encoder step. During the decoder step the encoder hidden state is fed into the decoder LSTM including the future known inputs. The model always predicts incrementally in the sense that for example to predict a 3 step ahead forecast it firsts predicts 1 and 2 step forecast and uses both forecasts to then predict the 3 step prediction. Both model architectures were used from the pytorch-forecasting library. The models were retrained for the different lead times. The used hyperparameters for both models are shown in Table \@ref(tab:tab-1).

```{r tab-1, echo=FALSE}
hyperparameter_data <- data.frame(
  Hyperparameter = c("Batch Size", "Epochs", "Hidden Size", "Attention Head Size", "Learning Rate",
                     "Dropout", "Weight Decay", "Gradient Clipping", "Loss Function", "Optimizer",
                     "Reduce on Plateau Patience", "Time Varying Known Features", "Time Varying Unknown Features"),
  LSTM = c(128, 100, 128, "-", 0.001, 0.2, 0.001, 0.1, "Mean Absolute Error", "Adam", 7, "t2m, tp", "Streamflow 15207507"),
  TFT = c(128, 80, 128, 2, 0.003, 0.1, 0.0001, 0.1, "Mean Absolute Error", "Adam", 7, "t2m, tp", "Streamflow 15207507")
)
knitr::kable(hyperparameter_data, caption = "Hyperparameter comparison between LSTM and TFT.")
```

### Results

The models show good performance for lead times of 1 and 2 days especially considering since the training and validation loss that is used is the MAE and not the KGE. The performance declines sharply for the LSTM model across the lead times while the decline for the TFT is more gradual as can be seen in Table \@ref(tab:tab-2).

```{r tab-2, echo=FALSE}
library(knitr)
performance_data <- data.frame(
  `Lead Time` = 1:7,
  `TFT KGE` = c('0.8352', '0.7103', '0.6410', '**0.6096**', '**0.5901**', '**0.5778**', '**0.5717**'),
  `LSTM KGE` = c('**0.9696**', '**0.8821**', '**0.6716**', '0.4943', '0.4302', '0.3312', '0.3185')
)
knitr::kable(performance_data, caption = "Performance comparison between TFT and LSTM models across different lead times. Better performing model for each lead time in bold", format = "markdown")
```

The forecasting the peaks of the streamflows is especially challenging and neither model performs particularly well on this task. Especially when considering that the peaks were drastically reduced due to the moving average smoothing of the target variable. This behaviour can be observed on most lead times here shown for a lead time of 5 days for the LSTM Model and the TFT Model in figure \@ref(fig:7) and in figure \@ref(fig:8) respectively.

```{r 7, cache=FALSE, out.width="500px", fig.cap = 'LSTM predicted streamflows for a lead time of 5 days (orange) compared to the observed streamflows (blue)', fig.align="center", echo=FALSE, eval = TRUE}
knitr::include_graphics('work/07-hydroLSTM/images/lag5_lstm.png')
```

```{r 8, cache=FALSE, out.width="500px", fig.cap = 'TFT predicted streamflows for a lead time of 5 days (orange) compared to the observed streamflows (blue)', fig.align="center", echo=FALSE, eval = TRUE}
knitr::include_graphics('work/07-hydroLSTM/images/lag5_tft.png')
```

## Conclusion

In this study, we compared the performance of LSTM (Long Short-Term Memory) and Temporal Fusion Transformer (TFT) models in forecasting streamflows up to seven days ahead, using precipitation and temperature as future known covariates alongside historical streamflow data.

Our findings indicate that both models exhibit limitations in predicting extreme values such as floods, with KGE (Kling-Gupta Efficiency) scores significantly lower than those reported in similar studies, likely due to reduced data availability and the challenges inherent in modeling a river system instead of a reservoir. The results also demonstrate a clear difference in performance trends between the two models across different lead times.

The LSTM model's KGE scores decline sharply with increasing lead time, starting at 0.9696 for a one-day lead time and dropping to 0.3185 for a seven-day lead time. Conversely, the TFT model shows a more gradual decline, from 0.8352 at one day to 0.5717 at seven days, suggesting it maintains more consistent accuracy over longer forecast horizons.

Despite the sharper decline in performance for longer lead times, the LSTM model is notably less resource-dependent, making it a viable option for scenarios where computational resources are limited. However, our attempts to forecast streamflows without future known meteorological variables were unsuccessful, underscoring the importance of these covariates in achieving accurate predictions.

In summary, while TFT demonstrates better stability over extended forecast periods, the LSTM model offers advantages in resource efficiency. Future work should explore strategies to improve model robustness for extreme value prediction and investigate additional data sources to enhance forecast accuracy.

## Outlook

Firstly, implementing a robust hyperparameter tuning routine is essential to optimize model performance. This process will require additional computational resources due to the complexity and extensive search space. Given the high dependency of hyperparameters on lag, it is crucial to carefully adjust them to capture the temporal dynamics accurately.

Improving data preprocessing is another critical aspect. This includes downscaling meteorological variables to ensure the input data aligns better with the spatial and temporal resolution required for accurate streamflow predictions. Performance optimization techniques will also be explored to streamline the computational efficiency of the models.

To make the models more sensitive to extreme events such as floods, specialized training techniques and loss functions will be employed. Experimentation with different model architectures could offer insights into more effective structures for capturing the complex patterns in river streamflow data.

The models need to generalize across different rivers, which involves accounting for diverse hydrological and geographical characteristics. Including static variables such as river basin properties and land use information will help in creating a more comprehensive model that can adapt to various river systems.

While the moving average is used in @sabzipour, it is not advised to use it for streamflow forecasting. By reducing the peaks in the target variable it artificially boosts the predictive abitlity of the model, but also forces the model to miss the peaks by an even larger margin. In a flood scenario even with a high KGE, the model would always miss the exact streamflow value as it was trained on lower peaks and would underestimate the posed danger in the situation.
