{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.metrics import MAE\n",
    "import os\n",
    "import pandas as pd # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>measurement_location</th>\n",
       "      <th>Datum</th>\n",
       "      <th>15202300</th>\n",
       "      <th>15205501</th>\n",
       "      <th>15207507</th>\n",
       "      <th>15210206</th>\n",
       "      <th>15212008</th>\n",
       "      <th>15212700</th>\n",
       "      <th>15213500</th>\n",
       "      <th>15214003</th>\n",
       "      <th>15214604</th>\n",
       "      <th>15216009</th>\n",
       "      <th>15217908</th>\n",
       "      <th>15221009</th>\n",
       "      <th>15228008</th>\n",
       "      <th>15241006</th>\n",
       "      <th>15242304</th>\n",
       "      <th>15243001</th>\n",
       "      <th>15246000</th>\n",
       "      <th>15247002</th>\n",
       "      <th>15247501</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>14.4</td>\n",
       "      <td>17.2</td>\n",
       "      <td>19.8</td>\n",
       "      <td>3.87</td>\n",
       "      <td>8.66</td>\n",
       "      <td>9.66</td>\n",
       "      <td>0.433</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.207</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.671</td>\n",
       "      <td>2.59</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.870</td>\n",
       "      <td>1.26</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>14.4</td>\n",
       "      <td>17.0</td>\n",
       "      <td>19.3</td>\n",
       "      <td>3.78</td>\n",
       "      <td>8.52</td>\n",
       "      <td>9.46</td>\n",
       "      <td>0.435</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.204</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.664</td>\n",
       "      <td>2.57</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.870</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-03</td>\n",
       "      <td>14.1</td>\n",
       "      <td>16.6</td>\n",
       "      <td>19.3</td>\n",
       "      <td>3.78</td>\n",
       "      <td>8.40</td>\n",
       "      <td>9.42</td>\n",
       "      <td>0.429</td>\n",
       "      <td>2.61</td>\n",
       "      <td>0.201</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.641</td>\n",
       "      <td>2.58</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.871</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-01-04</td>\n",
       "      <td>14.2</td>\n",
       "      <td>16.6</td>\n",
       "      <td>19.3</td>\n",
       "      <td>3.84</td>\n",
       "      <td>8.57</td>\n",
       "      <td>9.50</td>\n",
       "      <td>0.445</td>\n",
       "      <td>2.49</td>\n",
       "      <td>0.214</td>\n",
       "      <td>1.64</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.642</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.929</td>\n",
       "      <td>1.26</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-01-05</td>\n",
       "      <td>16.4</td>\n",
       "      <td>18.4</td>\n",
       "      <td>20.6</td>\n",
       "      <td>4.36</td>\n",
       "      <td>9.94</td>\n",
       "      <td>10.80</td>\n",
       "      <td>0.545</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.267</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.130</td>\n",
       "      <td>0.690</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.300</td>\n",
       "      <td>1.150</td>\n",
       "      <td>1.61</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3648</th>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>79.7</td>\n",
       "      <td>102.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>28.50</td>\n",
       "      <td>46.20</td>\n",
       "      <td>57.10</td>\n",
       "      <td>1.900</td>\n",
       "      <td>17.30</td>\n",
       "      <td>0.721</td>\n",
       "      <td>10.30</td>\n",
       "      <td>6.070</td>\n",
       "      <td>3.920</td>\n",
       "      <td>12.20</td>\n",
       "      <td>0.786</td>\n",
       "      <td>3.270</td>\n",
       "      <td>9.05</td>\n",
       "      <td>12.50</td>\n",
       "      <td>2.020</td>\n",
       "      <td>0.741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3649</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>71.4</td>\n",
       "      <td>82.5</td>\n",
       "      <td>97.7</td>\n",
       "      <td>26.00</td>\n",
       "      <td>41.90</td>\n",
       "      <td>51.30</td>\n",
       "      <td>1.710</td>\n",
       "      <td>15.60</td>\n",
       "      <td>0.801</td>\n",
       "      <td>9.85</td>\n",
       "      <td>5.480</td>\n",
       "      <td>3.470</td>\n",
       "      <td>10.90</td>\n",
       "      <td>0.653</td>\n",
       "      <td>2.890</td>\n",
       "      <td>7.23</td>\n",
       "      <td>9.97</td>\n",
       "      <td>1.750</td>\n",
       "      <td>0.619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3650</th>\n",
       "      <td>2023-12-30</td>\n",
       "      <td>64.0</td>\n",
       "      <td>71.4</td>\n",
       "      <td>86.3</td>\n",
       "      <td>24.40</td>\n",
       "      <td>38.60</td>\n",
       "      <td>46.90</td>\n",
       "      <td>1.530</td>\n",
       "      <td>14.70</td>\n",
       "      <td>0.835</td>\n",
       "      <td>9.40</td>\n",
       "      <td>4.900</td>\n",
       "      <td>3.100</td>\n",
       "      <td>9.77</td>\n",
       "      <td>0.557</td>\n",
       "      <td>2.550</td>\n",
       "      <td>5.92</td>\n",
       "      <td>8.54</td>\n",
       "      <td>1.470</td>\n",
       "      <td>0.514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>59.1</td>\n",
       "      <td>63.0</td>\n",
       "      <td>77.3</td>\n",
       "      <td>22.20</td>\n",
       "      <td>35.30</td>\n",
       "      <td>42.50</td>\n",
       "      <td>1.410</td>\n",
       "      <td>13.10</td>\n",
       "      <td>0.669</td>\n",
       "      <td>8.83</td>\n",
       "      <td>4.480</td>\n",
       "      <td>2.670</td>\n",
       "      <td>8.92</td>\n",
       "      <td>0.492</td>\n",
       "      <td>2.330</td>\n",
       "      <td>5.40</td>\n",
       "      <td>7.66</td>\n",
       "      <td>1.290</td>\n",
       "      <td>0.458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3652</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>56.0</td>\n",
       "      <td>58.5</td>\n",
       "      <td>71.9</td>\n",
       "      <td>20.70</td>\n",
       "      <td>33.30</td>\n",
       "      <td>40.30</td>\n",
       "      <td>1.420</td>\n",
       "      <td>12.10</td>\n",
       "      <td>0.565</td>\n",
       "      <td>8.57</td>\n",
       "      <td>4.540</td>\n",
       "      <td>2.390</td>\n",
       "      <td>8.46</td>\n",
       "      <td>0.457</td>\n",
       "      <td>2.250</td>\n",
       "      <td>6.30</td>\n",
       "      <td>8.20</td>\n",
       "      <td>1.260</td>\n",
       "      <td>0.414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3653 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "measurement_location       Datum  15202300  15205501  15207507  15210206  \\\n",
       "0                     2014-01-01      14.4      17.2      19.8      3.87   \n",
       "1                     2014-01-02      14.4      17.0      19.3      3.78   \n",
       "2                     2014-01-03      14.1      16.6      19.3      3.78   \n",
       "3                     2014-01-04      14.2      16.6      19.3      3.84   \n",
       "4                     2014-01-05      16.4      18.4      20.6      4.36   \n",
       "...                          ...       ...       ...       ...       ...   \n",
       "3648                  2023-12-28      79.7     102.0     120.0     28.50   \n",
       "3649                  2023-12-29      71.4      82.5      97.7     26.00   \n",
       "3650                  2023-12-30      64.0      71.4      86.3     24.40   \n",
       "3651                  2023-12-31      59.1      63.0      77.3     22.20   \n",
       "3652                  2024-01-01      56.0      58.5      71.9     20.70   \n",
       "\n",
       "measurement_location  15212008  15212700  15213500  15214003  15214604  \\\n",
       "0                         8.66      9.66     0.433      2.35     0.207   \n",
       "1                         8.52      9.46     0.435      2.51     0.204   \n",
       "2                         8.40      9.42     0.429      2.61     0.201   \n",
       "3                         8.57      9.50     0.445      2.49     0.214   \n",
       "4                         9.94     10.80     0.545      2.86     0.267   \n",
       "...                        ...       ...       ...       ...       ...   \n",
       "3648                     46.20     57.10     1.900     17.30     0.721   \n",
       "3649                     41.90     51.30     1.710     15.60     0.801   \n",
       "3650                     38.60     46.90     1.530     14.70     0.835   \n",
       "3651                     35.30     42.50     1.410     13.10     0.669   \n",
       "3652                     33.30     40.30     1.420     12.10     0.565   \n",
       "\n",
       "measurement_location  15216009  15217908  15221009  15228008  15241006  \\\n",
       "0                         1.66     0.921     0.671      2.59     0.253   \n",
       "1                         1.62     0.909     0.664      2.57     0.253   \n",
       "2                         1.60     0.907     0.641      2.58     0.253   \n",
       "3                         1.64     0.935     0.642      2.60     0.256   \n",
       "4                         1.80     1.130     0.690      3.10     0.300   \n",
       "...                        ...       ...       ...       ...       ...   \n",
       "3648                     10.30     6.070     3.920     12.20     0.786   \n",
       "3649                      9.85     5.480     3.470     10.90     0.653   \n",
       "3650                      9.40     4.900     3.100      9.77     0.557   \n",
       "3651                      8.83     4.480     2.670      8.92     0.492   \n",
       "3652                      8.57     4.540     2.390      8.46     0.457   \n",
       "\n",
       "measurement_location  15242304  15243001  15246000  15247002  15247501  \n",
       "0                        0.870      1.26      2.35     0.355     0.120  \n",
       "1                        0.870      1.23      2.32     0.361     0.131  \n",
       "2                        0.871      1.23      2.30     0.362     0.130  \n",
       "3                        0.929      1.26      2.44     0.382     0.127  \n",
       "4                        1.150      1.61      3.14     0.561     0.175  \n",
       "...                        ...       ...       ...       ...       ...  \n",
       "3648                     3.270      9.05     12.50     2.020     0.741  \n",
       "3649                     2.890      7.23      9.97     1.750     0.619  \n",
       "3650                     2.550      5.92      8.54     1.470     0.514  \n",
       "3651                     2.330      5.40      7.66     1.290     0.458  \n",
       "3652                     2.250      6.30      8.20     1.260     0.414  \n",
       "\n",
       "[3653 rows x 20 columns]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream = pd.read_pickle(os.path.join('..','data','processed','stream_processed.pkl'),compression= 'zip')\n",
    "meteo = pd.read_pickle(os.path.join('..','data','processed','meteo_processed.pkl'),compression = 'zip')\n",
    "stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          1\n",
       "2          2\n",
       "3          3\n",
       "4          4\n",
       "        ... \n",
       "3647    3647\n",
       "3648    3648\n",
       "3649    3649\n",
       "3650    3650\n",
       "3651    3651\n",
       "Name: date, Length: 3652, dtype: int64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream = stream.iloc[:3652]\n",
    "data = stream.join(meteo).drop(columns = 'Datum')\n",
    "data['groups'] = 0\n",
    "data['date'] = data.index\n",
    "data['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        19.8\n",
       "1        19.3\n",
       "2        19.3\n",
       "3        19.3\n",
       "4        20.6\n",
       "        ...  \n",
       "3647    161.0\n",
       "3648    120.0\n",
       "3649     97.7\n",
       "3650     86.3\n",
       "3651     77.3\n",
       "Name: 15207507, Length: 3652, dtype: float64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['15207507']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TimeseriesDataset(Dataset):   \n",
    "#     def __init__(self, X: np.ndarray,y: np.ndarray, seq_len : int, lag : int):\n",
    "#         super().__init__()\n",
    "#         self.X = torch.tensor(X).float()\n",
    "#         self.y = torch.tensor(y).float()\n",
    "#         self.seq_len = seq_len\n",
    "#         self.lag = lag\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.X.__len__() - (self.seq_len)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         return self.X[index:index+self.seq_len], self.X[index+self.seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class StreamFlowDataModule(pl.LightningDataModule):\n",
    "#     def __init__(self, X : np.array, num_workers = 0,seq_len = 364, batch_size=16):\n",
    "#         super().__init__()\n",
    "#         self.X = X\n",
    "#         self.seq_len = seq_len\n",
    "#         self.batch_size = batch_size\n",
    "#         self.num_workers = num_workers\n",
    "\n",
    "#     def prepare_data(self):\n",
    "#         self.X_val = self.X[0:int(len(self.X)*0.15)]\n",
    "#         self.X_train = self.X[int(len(self.X)*0.15)+1:int(len(self.X)*0.85)]\n",
    "#         self.X_test = self.X[int(len(self.X)*0.85)+1:]\n",
    "        \n",
    "#     def train_dataloader(self):\n",
    "#         train_dataset = TimeseriesDataset(self.X_train, \n",
    "#                                           seq_len=self.seq_len)\n",
    "        \n",
    "#         train_loader = DataLoader(train_dataset,\n",
    "#                                   batch_size = self.batch_size, \n",
    "#                                   shuffle = True, \n",
    "#                                   num_workers = self.num_workers)\n",
    "#         return train_loader\n",
    "    \n",
    "#     def val_dataloader(self):\n",
    "#         val_dataset = TimeseriesDataset(self.X_val, \n",
    "#                                         seq_len=self.seq_len)\n",
    "        \n",
    "#         val_loader = DataLoader(val_dataset, \n",
    "#                                 batch_size = self.batch_size, \n",
    "#                                 shuffle = False, \n",
    "#                                 num_workers = self.num_workers)\n",
    "#         return val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data.columns[20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3652, 94) (3652, 1)\n"
     ]
    }
   ],
   "source": [
    "a = data[cols].to_numpy()\n",
    "b = data['15207507'].to_numpy()\n",
    "print(a.shape,b[:,np.newaxis].shape)\n",
    "c = np.concatenate((a,b[:,np.newaxis]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMModel(pl.LightningModule):\n",
    "#     def __init__(self, input_size, hidden_size, seq_len ,dropout, num_layers,criterion,learning_rate = 0.0001):\n",
    "#         super(LSTMModel, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.input_size  = input_size\n",
    "#         self.seq_len = seq_len\n",
    "#         self.dropout  = dropout\n",
    "#         self.criterion = criterion\n",
    "#         self.learning_rate = learning_rate\n",
    "\n",
    "        \n",
    "#         self.lstm = nn.LSTM(input_size=self.input_size, \n",
    "#                             hidden_size=self.hidden_size,\n",
    "#                             num_layers=num_layers, \n",
    "#                             dropout=dropout, \n",
    "#                             batch_first=True)\n",
    "        \n",
    "#         self.linear = nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # lstm_out = (batch_size, seq_len, hidden_size)\n",
    "#         lstm_out, _ = self.lstm(x.unsqueeze(dim = 2))\n",
    "#         y_pred = self.linear(lstm_out[:,-1])\n",
    "#         return y_pred\n",
    "    \n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         x, y = batch\n",
    "#         y_hat = self(x)\n",
    "#         loss = self.criterion(y_hat, y)\n",
    "#         self.log('train_loss', loss,on_step=True, on_epoch=True, prog_bar=True)\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         x, y = batch\n",
    "#         y_hat = self(x)\n",
    "#         loss = self.criterion(y_hat, y)\n",
    "#         self.log('val_loss', loss,on_epoch = True, prog_bar = True)\n",
    "#         return loss\n",
    "    \n",
    "#     def configure_optimizers(self):\n",
    "#         return torch.optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = pl.Trainer(\n",
    "#         max_epochs=7,\n",
    "#         accelerator = 'cpu',\n",
    "#         precision=16,\n",
    "#         fast_dev_run= False\n",
    "#     )\n",
    "\n",
    "#     # Example usage\n",
    "# input_size = 1  # Assuming univariate time series data\n",
    "# hidden_size = 16\n",
    "# output_size = 1  # Assuming predicting one step ahead\n",
    "# batch_size = 64\n",
    "# seq_len = 364\n",
    "# dropout = 0\n",
    "# num_layers = 32\n",
    "# criterion = nn.L1Loss()\n",
    "\n",
    "# model = LSTMModel(input_size, hidden_size, dropout= dropout, seq_len=seq_len,num_layers=num_layers,criterion = criterion)\n",
    "\n",
    "# dm = StreamFlowDataModule(X = data.to_numpy(),batch_size=batch_size)\n",
    "# # dm.prepare_data()\n",
    "# # first_batch = next(iter(dm.train_dataloader()))[0].unsqueeze(dim = 2)\n",
    "# # print(first_batch)\n",
    "# trainer.fit(model, datamodule= dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pred_len = 6\n",
    "training_cutoff = data[\"date\"].max() - max_pred_len\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data.iloc[:training_cutoff],\n",
    "    target='15207507',\n",
    "    group_ids=['groups'],\n",
    "    time_idx='date',\n",
    "    max_encoder_length=364,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_pred_len,\n",
    "    time_varying_unknown_reals=['15207507'],\n",
    "    time_varying_known_reals=data.columns[20:-1].values.tolist()\n",
    ")\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dataset to a dataloader\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=32, num_workers=2)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=32, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lemarx/anaconda3/envs/Climate_Data/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Users/lemarx/anaconda3/envs/Climate_Data/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/lemarx/anaconda3/envs/Climate_Data/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Missing logger folder: /Users/lemarx/Documents/01_projects/Seminar_ClimateNStatistics/work/07-hydroLSTM/notebooks/lightning_logs\n",
      "/Users/lemarx/anaconda3/envs/Climate_Data/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(192.1167, device='mps:0')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n",
    "# MAE()(baseline_predictions.output, baseline_predictions.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/lemarx/anaconda3/envs/Climate_Data/lib/python3.10/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/lemarx/anaconda3/envs/Climate_Data/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Users/lemarx/anaconda3/envs/Climate_Data/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:199: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    max_epochs=3,\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=64,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=1,\n",
    "    dropout=0.2,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=64,  # set to <= hidden_size\n",
    "    loss=MAE(),\n",
    "    optimizer=\"Ranger\"\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    # reduce_on_plateau_patience=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | MAE                             | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 0     \n",
      "3  | prescalers                         | ModuleDict                      | 12.3 K\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 0     \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 2.0 M \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 2.0 M \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 16.8 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 16.8 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 16.8 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 16.8 K\n",
      "11 | lstm_encoder                       | LSTM                            | 33.3 K\n",
      "12 | lstm_decoder                       | LSTM                            | 33.3 K\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 8.3 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 128   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 20.9 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 16.6 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 8.4 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 16.8 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 8.4 K \n",
      "20 | output_layer                       | Linear                          | 65    \n",
      "----------------------------------------------------------------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "17.012    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lemarx/anaconda3/envs/Climate_Data/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lemarx/anaconda3/envs/Climate_Data/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  14%|█▎        | 14/102 [00:34<03:35,  0.41it/s, v_num=3, train_loss_step=11.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lemarx/anaconda3/envs/Climate_Data/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Climate_Data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
